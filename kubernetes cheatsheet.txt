
minikube version

minikube start			: start minikube

kubectl version

kubectl cluster-info 		: We have a running master and a dashboard. The Kubernetes dashboard allows you to view your applications in a UI.

kubectl get nodes		: This command shows all nodes that can be used to host our applications


kubectl get nodes --help	: You can use --help after the command to get additional info about possible parameters


kubectl run kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --port=8080	: The run command creates a new deployment. We need to provide the deployment name and app image location (include the full repository url for images hosted outside Docker hub). We want to run the app on a specific port so we add the --port parameter


kubectl get deployments		: To list your deployments use the get deployments command


kubectl proxy			: The kubectl command can create a proxy that will forward communications into the cluster-wide, private network. The proxy can be terminated by pressing control-C and won't show any output while its running.
				  We now have a connection between our host (the online terminal) and the Kubernetes cluster. The proxy enables direct access to the API from these terminals.
				  You can see all those APIs hosted through the proxy endpoint, now available at through http://localhost:8001


curl http://localhost:8001/version	: We can query the version directly through the API using the curl command


export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')	: The API server will automatically create an endpoint for each pod, based on the pod name, that is also accessible through the proxy.
															  First we need to get the Pod name, and we'll store in the environment variable POD_NAME

echo Name of the Pod: $POD_NAME


curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy/	: Now we can make an HTTP request to the application running in that pod


kubectl get pods		: Look for existing Pods


kubectl describe pods		: To view what containers are inside that Pod and what images are used to build those containers.
				  details about the Pod’s container: IP address, the ports used and a list of events related to the lifecycle of the Pod.

kubectl logs $POD_NAME		: View the container logs


Executing command on the container


kubectl exec $POD_NAME env	: List the environment variables. Worth mentioning that the name of the container itself can be omitted since we only have a single container in the Pod


kubectl exec -ti $POD_NAME bash	: Start a bash session in the Pod’s container

	> cat server.js
	> curl localhost:8080



Kubernetes Service - Exposing Your App


kubectl get services		: List the current Services from our cluster


kubectl expose deployment/kubernetes-bootcamp --type="NodePort" --port 8080	: To create a new service and expose it to external traffic we’ll use the expose command with NodePort as parameter (minikube does not support the LoadBalancer option yet)
kubectl expose deployment/kubernetes-bootcamp --type="LoadBalancer" --port 8080	: You can also use LoadBalancer if you like

kubectl get services

kubectl describe services/kubernetes-bootcamp	: To find out what port was opened externally (by the NodePort option) we’ll run the describe service command

export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')	: Create an environment variable called NODE_PORT that has the value of the Node port assigned
echo NODE_PORT=$NODE_PORT


curl $(minikube ip):$NODE_PORT		: Now we can test that the app is exposed outside of the cluster using  curl, the IP of the Node and the externally exposed port
     or VM's IP




Using Labels


kubectl describe deployment			: The Deployment created automatically a label for our Pod. With describe deployment command you can see the name of the label


kubectl get pods -l run=kubernetes-bootcamp	: Let's use this label to query our list of Pods. We’ll use the kubectl get pods command with -l as a parameter, followed by the label values


kubectl get services -l run=kubernetes-bootcamp	: You can do the same to list the existing services


export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')	: Get the name of the Pod and store it in the POD_NAME environment variable
echo Name of the Pod: $POD_NAME


kubectl label pod $POD_NAME app=v1		: To apply a new label we use the label command followed by the object type, object name and the new label


kubectl describe pods $POD_NAME			: The previous command will apply a new label to our Pod (we pinned the application version to the Pod), and we can check it with the describe pod command


kubectl get pods -l app=v1			: We can query now the list of pods using the new label




Deleting a service


kubectl delete service -l run=kubernetes-bootcamp	: To delete Services you can use the delete service command. Labels can be used also here


kubectl get services					: Confirm that the service is gone


curl $(minikube ip):$NODE_PORT				: To confirm that route is not exposed anymore you can curl the previously exposed IP and port
							  This proves that the app is not reachable anymore from outside of the cluster


kubectl exec -ti $POD_NAME curl localhost:8080		: You can confirm that the app is still running with a curl inside the pod





------------------------------------------
Creating and starting a kubernetes cluster
------------------------------------------

kubeadm init			: Starting a Cluster - Kubernetes (using kubeadm)

kubeadm join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<hash>	: To join another node one must simply run the outputted command in another node

kubectl apply -f https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml	: Installing a pod network. Docker swarm comes bundled with a service mesh that provides networking capabilities inside the cluster. While this is convenient, Kubernetes comes with more flexibility in this space, letting you install a network of your choice. The official implementations include Calico, Canal, Flannel, Kube-Router, Romana and Weave Net. The process of installing either of them is more of the same, but I’ll stick with Calico for this tutorial.


---------------------------------------
Running a Service - Kubernetes (inline)
---------------------------------------

kubectl run nginx --image=nginx:latest

kubectl expose deployment nginx --port 80 --type NodePort

kubectl get services


----------------------------------------------------------------
Running a Service - Docker Swarm (YAML) OR (docker stack deploy)
----------------------------------------------------------------

kubectl apply -f nginx.yml : like docker stack deploy

yaml file src: https://hackernoon.com/a-kubernetes-guide-for-docker-swarm-users-c14c8aa266cc

Explanation:
Because it’s built around a more modular architecture, Kubernetes requires two resources to achieve the same functionality that Swarm has: A Depoyment and a Service.
A Deployment pretty much defines the characteristics of a service. It is where containers, volumes, secrets and configurations are defined. Deployments also define the number of replicas, and the replication and placement strategies. You can see them as the equivalent of a stack definition in swarm, minus load balancing.
In fact, deployment’s are a higher-level abstraction over lower-level Kubernetes resources suh as Pods and Replica Sets. Everything defined in the template part of the deployment definition defines a pod, which is the smallest unit of scheduling that Kubernetes provides. A pod does not equal to a container. It’s a set of resources that are meant to be scheduled together; for example a Container and a Volume, or two Containers. In most cases, a Pod will contain only one container, but it’s important to understand that difference.
The second part of the file defines a Service resource, which can be seen as a way to refer to a set of pods in the network and load balance between them. The type NodePort tells Kubernetes to assign a externally-accessible port on every node of the cluster (the same on all nodes). This is what swarm did as well. You tell Services what to load balance between by using selectors, and this is why labeling is so important in Kubernetes.
In this case, Kubernetes is much more powerful: For example, you can define a service of type LoadBalancer, which will spawn a Load Balancer in your cloud provider (prior configuration), such as an ELB in AWS, which will point to your service. The default service type is ClusterIP, which defines a service that can be accessed anywhere in the cluster on a given port, but not externally. Using ClusterIP is equal to defining a service without an external mapping in Swarm.


----------------
Creating volumes
----------------

In Docker:

version: '3'
services:
  nginx:
    image: nginx:latest
    ports:
      - 80:80
    volumes:
      - nginx-volume:/srv/www
    deploy:
      mode: replicated
      replicas: 1
volumes:
  nginx-volume:


In Kubernetes

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /srv/www
          name: nginx-volume
       volumes:
         - name: nginx-volume
           emptyDir: {}
---
apiVersion: v1
kind: Service
type: NodePort
metadata:
  name: nginx
spec:
  selector:
    app: nginx
  ports:
  - port: 80



-------
SCALING
-------

kubectl get deployments     					: To list your deployments

kubectl scale deployments/kubernetes-bootcamp --replicas=4	: scale the Deployment to 4 replicas. We’ll use the kubectl scale command, followed by the deployment type, name and desired number of instances

kubectl get deployments     					: list your Deployments once again

kubectl get pods -o wide     					: check if the number of Pods changed. There are 4 Pods now, with different IP addresses

kubectl describe deployments/kubernetes-bootcamp     		: The change was registered in the Deployment events log. To check that, use the describe command


--------------
LOAD BALANCING
--------------

kubectl describe services/kubernetes-bootcamp     		: check that the Service is load-balancing the traffic. To find out the exposed IP and Port we can use the describe service

export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')
echo NODE_PORT=$NODE_PORT     					: Create an environment variable called NODE_PORT that has a value as the Node port

curl $(minikube ip):$NODE_PORT     				: we’ll do a curl to the exposed IP and port. Execute the command multiple times. We hit a different Pod with every request. This demonstrates that the load-balancing is working

you can also get IP with this

export NODE_IP=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.clusterIP)}}')
echo NODE_IP=$NODE_IP


----------
SCALE DOWN
----------

kubectl scale deployments/kubernetes-bootcamp --replicas=2	: scale down the Service to 2 replicas, run again the scale command

kubectl get deployments     					: List the Deployments to check if the change was applied with the get deployments command

kubectl get pods -o wide     					: The number of replicas decreased to 2. List the number of Pods, with get pods. This confirms that 2 Pods were terminated


-----------------------------
UPDATE THE VERSION OF THE APP
-----------------------------

kubectl get deployments     					: list your deployments use the get deployments command

kubectl get pods     						: list the running Pods use the get pods command

kubectl describe pods     					: view the current image version of the app, run a describe command against the Pods (look at the Image field)

kubectl describe pods/kubernetes-bootcamp-5c69669756-krdzj      : describe a single specific pod

kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2		: update the image of the application to version 2, use the set image command, followed by the deployment name and the new image version

kubectl get pods     						: The previous command notified the Deployment to use a different image for your app and initiated a rolling update. Check the status of the new Pods, and view the old one terminating with the get pods command


----------------
VERIFY AN UPDATE
----------------

kubectl describe services/kubernetes-bootcamp			: First, let’s check that the App is running. To find out the exposed IP and Port we can use describe service

export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')
echo NODE_PORT=$NODE_PORT					: Create an environment variable called NODE_PORT that has the value of the Node port assigned

curl $(minikube ip):$NODE_PORT					: Next, we’ll do a curl to the the exposed IP and port. We hit a different Pod with every request and we see that all Pods are running the latest version (v2).

kubectl rollout status deployments/kubernetes-bootcamp		: The update can be confirmed also by running a rollout status command. kubectl describe pods

kubectl describe pods						: To view the current image version of the app, run a describe command against the Pods. We run now version 2 of the app (look at the Image field)



------------------
ROLLBACK AN UPDATE
------------------

kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=gcr.io/google-samples/kubernetes-bootcamp:v10	: Let’s perform another update, and deploy image tagged as v10

kubectl get deployments						: Use get deployments to see the status of the deployment

kubectl describe pods						: There is no image called v10 in the repository

kubectl rollout undo deployments/kubernetes-bootcamp		: Let’s roll back to our previously working version. We’ll use the rollout undo command. The rollout command reverted the deployment to the previous known state (v2 of the image). Updates are versioned and you can revert to any previously know state of a Deployment.

kubectl get pods						: List again the Pods. Four Pods are running. Check again the image deployed on the them

kubectl describe pods						: We see that the deployment is using a stable version of the app (v2). The Rollback was successful



-------------------------------
Overview of Kubernetes Services
-------------------------------

Kubernetes Pods are mortal. Pods in fact have a lifecycle. When a worker node dies, the Pods running on the Node are also lost. A ReplicationController might then dynamically drive the cluster back to desired state via creation of new Pods to keep your application running. As another example, consider an image-processing backend with 3 replicas. Those replicas are fungible; the front-end system should not care about backend replicas or even if a Pod is lost and recreated. That said, each Pod in a Kubernetes cluster has a unique IP address, even Pods on the same Node, so there needs to be a way of automatically reconciling changes among Pods so that your applications continue to function.

A Service in Kubernetes is an abstraction which defines a logical set of Pods and a policy by which to access them. Services enable a loose coupling between dependent Pods. A Service is defined using YAML (preferred) or JSON, like all Kubernetes objects. The set of Pods targeted by a Service is usually determined by a LabelSelector (see below for why you might want a Service without including selector in the spec).

Although each Pod has a unique IP address, those IPs are not exposed outside the cluster without a Service. Services allow your applications to receive traffic. Services can be exposed in different ways by specifying a type in the ServiceSpec:

ClusterIP (default) - Exposes the Service on an internal IP in the cluster. This type makes the Service only reachable from within the cluster.
NodePort - Exposes the Service on the same port of each selected Node in the cluster using NAT. Makes a Service accessible from outside the cluster using <NodeIP>:<NodePort>. Superset of ClusterIP.
LoadBalancer - Creates an external load balancer in the current cloud (if supported) and assigns a fixed, external IP to the Service. Superset of NodePort.
ExternalName - Exposes the Service using an arbitrary name (specified by externalName in the spec) by returning a CNAME record with the name. No proxy is used. This type requires v1.7 or higher of kube-dns.
More information about the different types of Services can be found in the Using Source IP tutorial. Also see Connecting Applications with Services.

Additionally, note that there are some use cases with Services that involve not defining selector in the spec. A Service created without selector will also not create the corresponding Endpoints object. This allows users to manually map a Service to specific endpoints. Another possibility why there may be no selector is you are strictly using type: ExternalName.

Summary
Exposing Pods to external traffic
Load balancing traffic across multiple Pods
Using labels
A Kubernetes Service is an abstraction layer which defines a logical set of Pods and enables external traffic exposure, load balancing and service discovery for those Pods.


